{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "File data\\train.tsv is already downloaded.\nFile data\\validation.tsv is already downloaded.\nFile data\\test.tsv is already downloaded.\nFile data\\test_embeddings.tsv is already downloaded.\n"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. You need to download it by following this [link](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('week3\\GoogleNews-vectors-negative300.bin', binary=True, limit=500000) ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "These embeddings look good.\nC:\\Users\\rosal\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    result = np.zeros(dim)\n",
    "    cnt = 0\n",
    "    words = question.split()\n",
    "    for word in words:\n",
    "        if word in embeddings:\n",
    "            result += np.array(embeddings[word])\n",
    "            cnt += 1\n",
    "    if cnt != 0:\n",
    "        result /= cnt\n",
    "    return result \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        \n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Basic tests are passed.\n"
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\rosal\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "92   0.02582804  0.15026855 -0.0396169   0.09155273 -0.00744629\n -0.07728068  0.03527832  0.05577087 -0.08256022  0.07438151  0.07770284\n  0.04602051  0.14483134  0.07318115 -0.04928589 -0.03621419 -0.02388509\n  0.08150228  0.06958008  0.03169759  0.06756592  0.12858073 -0.06976318\n -0.06439209 -0.01879374  0.0004069  -0.07097117  0.0168457  -0.01023356\n -0.04824829  0.14086914  0.06515757  0.03277588 -0.22961426  0.02445475\n -0.05289714  0.13297526 -0.10970052 -0.01740519 -0.03198242 -0.0300293\n  0.01769416 -0.07728068 -0.0124766  -0.07100423 -0.05620321  0.04099528\n  0.10560826  0.02303641  0.03011649  0.08778599 -0.15618025  0.01663644\n -0.0181863  -0.14592634  0.06152344  0.03001186  0.08994838 -0.09877232\n  0.04105486 -0.0501709  -0.096235    0.09277344  0.00608608  0.07373047\n -0.05293492 -0.0884225  -0.05882045  0.12032645 -0.04644339 -0.02699498\n -0.07576643 -0.10993304  0.00097874  0.10470145  0.06086077  0.03353446\n -0.00167411 -0.06237793  0.05377197  0.01473563  0.01933943 -0.0037493\n -0.01484898  0.03843471  0.10944475  0.11579241  0.04893276  0.02664948\n  0.0453404   0.07082694  0.01202393 -0.0177002   0.07224819 -0.02660261\n  0.03641619  0.06401716 -0.1464495   0.00881849  0.05474854 -0.14055525\n  0.01695033  0.09655435  0.00796945 -0.1018764   0.02626256 -0.11342076\n -0.00735256  0.07083566 -0.12149484 -0.07448033 -0.02668108 -0.04525321\n  0.04107012  0.20054408 -0.07446289  0.03198242 -0.02888533 -0.07625907\n  0.06018066  0.04586356 -0.10032436  0.04779925  0.04743304 -0.05483573\n  0.11950684  0.07667759  0.05535889  0.00820487  0.10604422  0.0531311\n  0.01208496 -0.08614676 -0.14805385  0.03974479  0.03215681  0.03119768\n -0.01126535 -0.06222534 -0.16332136 -0.02308873 -0.04124233  0.09439523\n  0.02102225  0.04961722  0.0270037   0.00569807  0.06261771 -0.05290876\n -0.08041818  0.00476946 -0.00571115 -0.03345598 -0.00830514  0.03826032\n -0.0328064  -0.15540423 -0.06317139 -0.01375907 -0.03519549  0.01139614\n  0.09800502  0.03587123 -0.03862653  0.04380362  0.14432199  0.00500488\n -0.05912127 -0.15228271 -0.08612932 -0.08530916 -0.01670619  0.10358538\n -0.00563485  0.04889788  0.07631574  0.05594308  0.05636161 -0.09315709\n -0.05302211  0.05224609  0.05555943 -0.07237026  0.01111276  0.01918248\n -0.0703125   0.12037877  0.04884556 -0.03889683  0.03696987  0.1201695\n -0.03808594 -0.0023019  -0.00458636  0.02331543 -0.0816476  -0.0171596\n  0.18865095 -0.02131435 -0.0518886   0.03380476  0.05186244 -0.07945033\n -0.0365775  -0.0397219   0.05758231 -0.13214983 -0.03337751 -0.00050572\n  0.06832014 -0.00418527  0.00863211  0.03745815  0.02454133 -0.06207711\n -0.02034215  0.08910043 -0.02671596  0.04530552 -0.09085519 -0.06100028\n  0.07277134  0.03182765  0.00653512 -0.14449637  0.02908761  0.00510733\n  0.05408151 -0.01890128 -0.01374708 -0.01834542 -0.00082833  0.02669852\n  0.01895687  0.03756278  0.00800432  0.01028769  0.0264195  -0.00479562\n -0.01565116  0.05487932 -0.16829572  0.02556501 -0.03887068  0.02347238\n -0.05064174 -0.10567801  0.10588728  0.07374791 -0.08699908 -0.05027553\n -0.15131487 -0.02881731 -0.01850237  0.02577427 -0.03502982 -0.00100272\n -0.02842058 -0.01225935 -0.04951695  0.05611747 -0.03851318  0.07568359\n  0.05322266 -0.04774693 -0.01939174  0.03616769  0.09151786  0.0237078\n  0.02632373  0.05544172  0.02357701 -0.05722863  0.09065465  0.0090362\n  0.06492397  0.05181885 -0.02239118 -0.0543736   0.06082589  0.01663644\n  0.01028878  0.01115853 -0.02218192 -0.0548924   0.11446708  0.00244141\n  0.08281708 -0.13529205 -0.00660052 -0.0231236   0.03650774  0.00034877\n  0.05590466  0.04350063  0.03672137 -0.11108398  0.06760951  0.00623431\n  0.05908639  0.05535017  0.05756487 -0.06037249 -0.01506696  0.01018415\n -0.03420585 -0.05137771  0.00591714 -0.04437256 -0.04448155 -0.01572963\n  0.06084333  0.07420131  0.09487479  0.09666661 -0.10494559 -0.02734375\n -0.03217425  0.11666434 -0.02179827  0.01259068  0.02786691 -0.06926618\n -0.02313232 -0.06226458 -0.05892508  0.00782994  0.05855887  0.01136998\n -0.01797049  0.09798758  0.08717782 -0.03240095 -0.08236258 -0.04129464\n  0.00861468  0.02109637 -0.05730765 -0.0133231   0.02209473  0.00249372\n -0.01082938 -0.05440848 -0.04275949  0.03494699  0.02946254  0.05943952]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.0337443   0.01899872\n -0.07285156]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.0337443   0.01899872\n -0.07285156]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.0333141  -0.03724324\n  0.0312389 ]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.0333141  -0.03724324\n  0.0312389 ]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.01029816 -0.0234375\n -0.01717224]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.01029816 -0.0234375\n -0.01717224]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05595398 -0.01420711\n -0.02566294]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05595398 -0.01420711\n -0.02566294]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.11343384  0.00490316\n  0.05623372]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.11343384  0.00490316\n  0.05623372]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.10657501  0.0072403\n -0.11642456]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.10657501  0.0072403\n -0.11642456]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.34960938  0.02652995\n  0.05451457]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.34960938  0.02652995\n  0.05451457]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05790358 -0.02194449\n  0.03406847]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05790358 -0.02194449\n  0.03406847]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.18702393 -0.0090332\n  0.03457031]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.18702393 -0.0090332\n  0.03457031]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.03842773  0.0394165\n  0.01484375]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.03842773  0.0394165\n  0.01484375]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.01135254  0.02616119\n  0.09661865]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.01135254  0.02616119\n  0.09661865]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.21512277  0.01128278\n  0.06920515]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.21512277  0.01128278\n  0.06920515]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.07660348 -0.02214704\n -0.04916818]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.07660348 -0.02214704\n -0.04916818]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.03938802  0.04667155\n  0.07413737]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.03938802  0.04667155\n  0.07413737]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05786133 -0.03212891\n  0.03218079]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05786133 -0.03212891\n  0.03218079]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.01441193  0.03393555\n -0.07049561]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.01441193  0.03393555\n -0.07049561]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.09373474 -0.13665771\n  0.05804443]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.09373474 -0.13665771\n  0.05804443]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.16346436 -0.03994141\n  0.12255859]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.16346436 -0.03994141\n  0.12255859]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.16751099 -0.12864176\n -0.02522786]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.16751099 -0.12864176\n -0.02522786]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.13248698 -0.05090586\n  0.0069987 ]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.13248698 -0.05090586\n  0.0069987 ]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05052694 -0.10070801\n  0.07918294]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05052694 -0.10070801\n  0.07918294]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.09871826  0.05703125\n -0.05462341]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.09871826  0.05703125\n -0.05462341]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.04964193  0.05285645\n  0.12955729]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.04964193  0.05285645\n  0.12955729]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.01374817 -0.04504395\n  0.02656555]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.01374817 -0.04504395\n  0.02656555]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.01367188 -0.09997559\n  0.08666992]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.01367188 -0.09997559\n  0.08666992]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.01461356  0.05428641\n  0.00822939]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.01461356  0.05428641\n  0.00822939]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.0625     -0.13250732\n  0.06567383]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.0625     -0.13250732\n  0.06567383]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.02319336 -0.03747559\n  0.06027222]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.02319336 -0.03747559\n  0.06027222]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.19256592 -0.17712402\n  0.15441895]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.19256592 -0.17712402\n  0.15441895]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.01803589 -0.07583618\n  0.01739502]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.01803589 -0.07583618\n  0.01739502]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.12784831  0.20442708\n  0.03662109]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.12784831  0.20442708\n  0.03662109]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.0424703  -0.02567546\n -0.05271573]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.0424703  -0.02567546\n -0.05271573]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.06730513 -0.0845115\n  0.05150951]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.06730513 -0.0845115\n  0.05150951]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.09758301  0.05018311\n -0.03758545]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.09758301  0.05018311\n -0.03758545]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.07669503 -0.00450788\n -0.0160784 ]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.07669503 -0.00450788\n -0.0160784 ]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.06666395 -0.07178413\n  0.04618327]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.06666395 -0.07178413\n  0.04618327]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.09158325 -0.03210449\n -0.12219238]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.09158325 -0.03210449\n -0.12219238]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.19091797 -0.16503906\n  0.12939453]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.19091797 -0.16503906\n  0.12939453]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.01864794 -0.06263563\n  0.01754422]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.01864794 -0.06263563\n  0.01754422]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.06714681 -0.03111979\n -0.04355265]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.06714681 -0.03111979\n -0.04355265]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.07810465 -0.0620931\n -0.04530843]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.07810465 -0.0620931\n -0.04530843]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.13867188 -0.38476562\n  0.359375  ]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.13867188 -0.38476562\n  0.359375  ]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.12416077 -0.01751709\n -0.03442383]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.12416077 -0.01751709\n -0.03442383]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.09704081 -0.00455729\n -0.01224772]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.09704081 -0.00455729\n -0.01224772]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.02138519 -0.06372833\n  0.06802368]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.02138519 -0.06372833\n  0.06802368]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.00225152 -0.05457899\n  0.03185357]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.00225152 -0.05457899\n  0.03185357]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.0795683   0.00367109\n  0.00062471]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.0795683   0.00367109\n  0.00062471]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.06167603 -0.0687542\n -0.03786564]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.06167603 -0.0687542\n -0.03786564]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.04356893 -0.00178019\n  0.03491211]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.04356893 -0.00178019\n  0.03491211]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.06015015 -0.02919579\n -0.09124756]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.06015015 -0.02919579\n -0.09124756]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.12882487  0.03523763\n  0.03153483]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.12882487  0.03523763\n  0.03153483]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.28613281 -0.07552083\n  0.02848307]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.28613281 -0.07552083\n  0.02848307]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05208333 -0.0234375\n  0.10400391]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05208333 -0.0234375\n  0.10400391]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.07333984  0.03969421\n -0.02568359]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.07333984  0.03969421\n -0.02568359]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.08839925 -0.07492065\n  0.00478787]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.08839925 -0.07492065\n  0.00478787]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.10925293  0.02619934\n  0.01953125]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.10925293  0.02619934\n  0.01953125]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.0871582   0.08358765\n -0.01675415]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.0871582   0.08358765\n -0.01675415]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.00517578 -0.04633789\n -0.00390625]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.00517578 -0.04633789\n -0.00390625]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.01013184  0.06445312\n -0.02966309]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.01013184  0.06445312\n -0.02966309]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05457306 -0.0175705\n -0.10473633]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05457306 -0.0175705\n -0.10473633]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.00419108 -0.05803426\n  0.0417277 ]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.00419108 -0.05803426\n  0.0417277 ]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05279541 -0.02131144\n -0.06713867]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05279541 -0.02131144\n -0.06713867]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05096436 -0.02747091\n  0.02929688]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05096436 -0.02747091\n  0.02929688]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.08614095 -0.0016276\n  0.24422201]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.08614095 -0.0016276\n  0.24422201]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.11309814 -0.02445984\n -0.00424194]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.11309814 -0.02445984\n -0.00424194]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.10117187  0.05552368\n -0.05605469]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.10117187  0.05552368\n -0.05605469]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.08192139 -0.04622803\n  0.07529297]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.08192139 -0.04622803\n  0.07529297]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05908203  0.10180664\n -0.12054443]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05908203  0.10180664\n -0.12054443]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.1170166  -0.07098389\n  0.04573364]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.1170166  -0.07098389\n  0.04573364]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.15120443 -0.12430827\n  0.08138021]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.15120443 -0.12430827\n  0.08138021]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.11663818  0.02326965\n  0.04776001]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.11663818  0.02326965\n  0.04776001]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.01062012  0.09130859\n -0.25      ]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.01062012  0.09130859\n -0.25      ]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.08485921 -0.02190653\n -0.01457723]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.08485921 -0.02190653\n -0.01457723]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.0585111  -0.13037109\n -0.18664551]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.0585111  -0.13037109\n -0.18664551]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.11425781 -0.04437256\n  0.05621338]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.11425781 -0.04437256\n  0.05621338]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.02870941  0.034729\n  0.15734863]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.02870941  0.034729\n  0.15734863]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.13566895 -0.0527832\n  0.04213867]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.13566895 -0.0527832\n  0.04213867]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.04649523 -0.05799696\n  0.0682373 ]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.04649523 -0.05799696\n  0.0682373 ]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.20317925 -0.16590712\n  0.12060547]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.20317925 -0.16590712\n  0.12060547]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.17773438 -0.07606252\n  0.14306641]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.17773438 -0.07606252\n  0.14306641]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.06869229 -0.02490651\n -0.09416199]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.06869229 -0.02490651\n -0.09416199]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.04989901 -0.00401306\n  0.00046609]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.04989901 -0.00401306\n  0.00046609]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.12060547  0.02905273\n -0.08251953]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.12060547  0.02905273\n -0.08251953]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.10634068  0.06776646\n -0.02064732]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.10634068  0.06776646\n -0.02064732]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.09250488 -0.0373291\n  0.0597168 ]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.09250488 -0.0373291\n  0.0597168 ]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.05209351  0.08573914\n -0.06135559]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.05209351  0.08573914\n -0.06135559]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.09192766 -0.04851423\n -0.04638672]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.09192766 -0.04851423\n -0.04638672]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.0421875  -0.0338623\n  0.06508789]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.0421875  -0.0338623\n  0.06508789]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.05751953  0.04293213\n -0.06430664]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.05751953  0.04293213\n -0.06430664]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.26185303 -0.13188477\n  0.01044922]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.26185303 -0.13188477\n  0.01044922]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.01926967  0.03965541\n -0.03569685]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.01926967  0.03965541\n -0.03569685]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.01044922  0.05230713\n -0.00738525]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.01044922  0.05230713\n -0.00738525]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.08239746  0.03330994\n  0.06781006]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.08239746  0.03330994\n  0.06781006]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.10316467  0.05218506\n  0.04980469]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.10316467  0.05218506\n  0.04980469]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.01396484  0.01888123\n  0.05961914]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.01396484  0.01888123\n  0.05961914]\nhola [ 0.01929389 -0.02872721  0.04605611 ...  0.08989258  0.05184021\n  0.01665039]\n[ 0.01929389 -0.02872721  0.04605611 ...  0.08989258  0.05184021\n  0.01665039]\nhola [ 0.01929389 -0.02872721  0.04605611 ... -0.0957489   0.02639771\n -0.02853394]\n[ 0.01929389 -0.02872721  0.04605611 ... -0.0957489   0.02639771\n -0.02853394]\n"
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "    print(\"hola\", question2vec_result)\n",
    "    print(question2vec_result)\n",
    "\n",
    "#grader.submit_tag('Question2Vec', util.array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question;\n",
    "                   length is a number of questions which we are looking for duplicates;\n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    return np.average(np.array(dup_ranks) <= np.array([k]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Basic test are passed.\n"
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    return np.average((np.array(dup_ranks) <= np.array([k]))*1./(np.log2(1. + np.array(dup_ranks)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Basic test are passed.\n"
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "#grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "#grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv') ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[(0.99, 1, 'Q2'), (0.2, 0, 'Q1'), (-0.3, 2, 'Q3')]\n[(1, 'Q2'), (0, 'Q1'), (2, 'Q3')]\n"
    }
   ],
   "source": [
    "sim = [0.2, .99, -.3]\n",
    "Qs = ['Q1', 'Q2', 'Q3']\n",
    "merged_list=list(zip(sim, range(3), Qs))\n",
    "sorted_list  = sorted(merged_list, key=lambda x: x[0], reverse=True)\n",
    "print(sorted_list)\n",
    "result = [(b,c) for a,b,c in sorted_list]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    q_vecs = np.array([question_to_vec(question, embeddings, dim) for i in range(len(candidates))])\n",
    "    cand_vecs = np.array([question_to_vec(candidate, embeddings, dim) for candidate in candidates])\n",
    "    cosines = np.array(cosine_similarity(q_vecs, cand_vecs)[0])\n",
    "    merged_list = list(zip(cosines, range(len(candidates)), candidates))\n",
    "    #print(merged_list)\n",
    "    sorted_list  = sorted(merged_list, key=lambda x: x[0], reverse=True)\n",
    "    result = [(b,c) for a,b,c in sorted_list]\n",
    "    \n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questionTweets = []\n",
    "    candidatesTweets = []\n",
    "    f = open(\"pruebaTweetsSimilares.tsv\",\"+w\",encoding=\"utf-8\")\n",
    "    cases = open(\"week3\\\\data\\\\testTweets.tsv\", encoding='utf-8').readlines()\n",
    "    fileSize = len(cases)\n",
    "    i=0\n",
    "    while i < fileSize:\n",
    "        try:\n",
    "            questionTweets.append(cases[i])\n",
    "            temp = []\n",
    "            for j in range(1,4):\n",
    "                i += j\n",
    "                temp.append(cases[i]) \n",
    "            candidatesTweets.append(temp)\n",
    "        except:\n",
    "            print(\"hola\")\n",
    "\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    ranks = []\n",
    "    for i in range(len(questionTweets)):\n",
    "        try:\n",
    "            f.write('tweet:'+'\\n')\n",
    "            f.write(questionTweets[i])\n",
    "            f.write('tweets similares:'+'\\n')\n",
    "            rank = rank_candidates(questionTweets[i],candidatesTweets[i],results)\n",
    "            for r in rank:\n",
    "                f.write(r[1])\n",
    "                f.write('\\n')\n",
    "        except:\n",
    "            print(\"hola2\")\n",
    "        \n",
    "    #for question, q_candidates, result in zip(questionTweets, candidatesTweets, results):\n",
    "        #ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        #print(ranks)\n",
    "    \n",
    "    #for i in range(len(questionTweets)):\n",
    "     #   f.write(\"question:\")\n",
    "      #  f.write(questionTweets[i])\n",
    "       # for c in ranks[i]:\n",
    "        #    for e in c:\n",
    "         #       f.write(\"-------------------------------------------------------\")\n",
    "          #      f.write(e)\n",
    "            \n",
    "        #if not np.all(ranks == result):\n",
    "            \n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "hola\nhola2\nBasic tests are passed.\n"
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.212 | Hits@   1: 0.212\n",
      "DCG@   5: 0.267 | Hits@   5: 0.315\n",
      "DCG@  10: 0.282 | Hits@  10: 0.363\n",
      "DCG@ 100: 0.320 | Hits@ 100: 0.552\n",
      "DCG@ 500: 0.353 | Hits@ 500: 0.811\n",
      "DCG@1000: 0.373 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\nHow to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\njQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    prepared_validation.append([text_prepare(sentence) for sentence in line])\n",
    "    ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DCG@   1: 0.310 | Hits@   1: 0.310\nDCG@   5: 0.380 | Hits@   5: 0.443\nDCG@  10: 0.397 | Hits@  10: 0.494\nDCG@ 100: 0.430 | Hits@ 100: 0.661\nDCG@ 500: 0.453 | Hits@ 500: 0.835\nDCG@1000: 0.470 | Hits@1000: 1.000\n"
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "prepare_file('data/train.tsv', 'data/prepared_train.tsv')\n",
    "prepare_file('data/test.tsv', 'data/prepared_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/prepared_test.tsv' ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "#grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/prepared_train.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/prepared_train.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040110  loss: 0.008932  eta: 0h12m  tot: 0h3m2s  (20.0%)%  lr: 0.049780  loss: 0.035054  eta: 0h16m  tot: 0h0m5s  (0.6%)h0m11s  (1.2%)8.0%  lr: 0.049179  loss: 0.023291  eta: 0h15m  tot: 0h0m15s  (1.6%)8.4%  lr: 0.049139  loss: 0.022844  eta: 0h16m  tot: 0h0m16s  (1.7%)%  lr: 0.048929  loss: 0.021393  eta: 0h15m  tot: 0h0m19s  (2.0%)10.4%  lr: 0.048889  loss: 0.020889  eta: 0h15m  tot: 0h0m19s  (2.1%)14.4%  lr: 0.048549  loss: 0.018580  eta: 0h15m  tot: 0h0m26s  (2.9%)15.2%  lr: 0.048448  loss: 0.018229  eta: 0h15m  tot: 0h0m28s  (3.0%)15m  tot: 0h0m31s  (3.4%)19.6%  lr: 0.048028  loss: 0.016440  eta: 0h14m  tot: 0h0m36s  (3.9%)0.015840  eta: 0h14m  tot: 0h0m40s  (4.3%)m  tot: 0h0m41s  (4.5%)24.7%  lr: 0.047538  loss: 0.015077  eta: 0h14m  tot: 0h0m45s  (4.9%)26.0%  lr: 0.047437  loss: 0.014704  eta: 0h14m  tot: 0h0m48s  (5.2%)26.8%  lr: 0.047397  loss: 0.014505  eta: 0h14m  tot: 0h0m49s  (5.4%)26.9%  lr: 0.047387  loss: 0.014486  eta: 0h14m  tot: 0h0m49s  (5.4%)%  lr: 0.047257  loss: 0.014242  eta: 0h14m  tot: 0h0m52s  (5.6%)0.014163  eta: 0h14m  tot: 0h0m53s  (5.8%)30.2%  lr: 0.047057  loss: 0.013932  eta: 0h14m  tot: 0h0m56s  (6.0%)33.1%  lr: 0.046897  loss: 0.013410  eta: 0h14m  tot: 0h1m1s  (6.6%)33.2%  lr: 0.046877  loss: 0.013414  eta: 0h14m  tot: 0h1m1s  (6.6%)34.0%  lr: 0.046807  loss: 0.013301  eta: 0h14m  tot: 0h1m3s  (6.8%)35.1%  lr: 0.046647  loss: 0.013196  eta: 0h14m  tot: 0h1m5s  (7.0%)38.3%  lr: 0.046306  loss: 0.012862  eta: 0h14m  tot: 0h1m10s  (7.7%)39.2%  lr: 0.046206  loss: 0.012755  eta: 0h14m  tot: 0h1m11s  (7.8%)%  lr: 0.046146  loss: 0.012677  eta: 0h14m  tot: 0h1m13s  (8.0%)40.4%  lr: 0.046106  loss: 0.012624  eta: 0h13m  tot: 0h1m13s  (8.1%)0.012525  eta: 0h13m  tot: 0h1m15s  (8.2%)41.4%  lr: 0.046006  loss: 0.012501  eta: 0h13m  tot: 0h1m15s  (8.3%)41.6%  lr: 0.045996  loss: 0.012471  eta: 0h13m  tot: 0h1m16s  (8.3%)43.0%  lr: 0.045866  loss: 0.012295  eta: 0h13m  tot: 0h1m19s  (8.6%)0.045846  loss: 0.012277  eta: 0h14m  tot: 0h1m19s  (8.6%)44.5%  lr: 0.045716  loss: 0.012140  eta: 0h14m  tot: 0h1m22s  (8.9%)46.5%  lr: 0.045536  loss: 0.011901  eta: 0h13m  tot: 0h1m25s  (9.3%)46.9%  lr: 0.045496  loss: 0.011846  eta: 0h13m  tot: 0h1m25s  (9.4%)47.1%  lr: 0.045466  loss: 0.011836  eta: 0h13m  tot: 0h1m26s  (9.4%)47.7%  lr: 0.045405  loss: 0.011773  eta: 0h13m  tot: 0h1m27s  (9.5%)48.8%  lr: 0.045265  loss: 0.011685  eta: 0h13m  tot: 0h1m29s  (9.8%)%  lr: 0.045205  loss: 0.011625  eta: 0h13m  tot: 0h1m30s  (9.9%)13m  tot: 0h1m31s  (10.0%)55.0%  lr: 0.044725  loss: 0.011149  eta: 0h13m  tot: 0h1m40s  (11.0%)55.2%  lr: 0.044695  loss: 0.011133  eta: 0h13m  tot: 0h1m40s  (11.0%)55.4%  lr: 0.044675  loss: 0.011122  eta: 0h13m  tot: 0h1m40s  (11.1%)55.9%  lr: 0.044625  loss: 0.011103  eta: 0h13m  tot: 0h1m41s  (11.2%)%  lr: 0.044575  loss: 0.011082  eta: 0h13m  tot: 0h1m42s  (11.3%)56.7%  lr: 0.044565  loss: 0.011039  eta: 0h13m  tot: 0h1m43s  (11.3%)56.9%  lr: 0.044545  loss: 0.011035  eta: 0h13m  tot: 0h1m43s  (11.4%)58.6%  lr: 0.044334  loss: 0.010939  eta: 0h13m  tot: 0h1m46s  (11.7%)64.9%  lr: 0.043654  loss: 0.010518  eta: 0h13m  tot: 0h1m57s  (13.0%)h13m  tot: 0h1m58s  (13.1%)66.5%  lr: 0.043474  loss: 0.010397  eta: 0h13m  tot: 0h2m0s  (13.3%)66.9%  lr: 0.043424  loss: 0.010376  eta: 0h13m  tot: 0h2m0s  (13.4%)67.8%  lr: 0.043363  loss: 0.010307  eta: 0h13m  tot: 0h2m2s  (13.6%)68.0%  lr: 0.043343  loss: 0.010309  eta: 0h12m  tot: 0h2m2s  (13.6%)%  lr: 0.043243  loss: 0.010239  eta: 0h12m  tot: 0h2m4s  (13.8%)71.3%  lr: 0.043063  loss: 0.010122  eta: 0h12m  tot: 0h2m9s  (14.3%)73.1%  lr: 0.042863  loss: 0.010017  eta: 0h12m  tot: 0h2m12s  (14.6%)74.5%  lr: 0.042703  loss: 0.009944  eta: 0h12m  tot: 0h2m15s  (14.9%)12m  tot: 0h2m17s  (15.2%)77.4%  lr: 0.042523  loss: 0.009805  eta: 0h12m  tot: 0h2m20s  (15.5%)77.6%  lr: 0.042493  loss: 0.009794  eta: 0h12m  tot: 0h2m21s  (15.5%)77.7%  lr: 0.042483  loss: 0.009790  eta: 0h12m  tot: 0h2m21s  (15.5%)0.009706  eta: 0h12m  tot: 0h2m25s  (15.9%)81.4%  lr: 0.042102  loss: 0.009638  eta: 0h12m  tot: 0h2m27s  (16.3%)0h12m  tot: 0h2m32s  (16.8%)87.0%  lr: 0.041472  loss: 0.009406  eta: 0h12m  tot: 0h2m38s  (17.4%)0.009381  eta: 0h12m  tot: 0h2m39s  (17.6%)92.2%  lr: 0.040891  loss: 0.009208  eta: 0h12m  tot: 0h2m47s  (18.4%)97.0%  lr: 0.040340  loss: 0.009042  eta: 0h12m  tot: 0h2m56s  (19.4%)99.4%  lr: 0.040160  loss: 0.008951  eta: 0h12m  tot: 0h3m1s  (19.9%)99.8%  lr: 0.040120  loss: 0.008937  eta: 0h12m  tot: 0h3m2s  (20.0%)\n",
      " ---+++                Epoch    0 Train error : 0.00892204 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030130  loss: 0.002629  eta: 0h8m  tot: 0h5m59s  (40.0%)%  lr: 0.039950  loss: 0.002057  eta: 0h9m  tot: 0h3m6s  (20.2%)1.4%  lr: 0.039920  loss: 0.002178  eta: 0h10m  tot: 0h3m7s  (20.3%)3.4%  lr: 0.039670  loss: 0.002664  eta: 0h11m  tot: 0h3m10s  (20.7%)9.9%  lr: 0.038999  loss: 0.002470  eta: 0h11m  tot: 0h3m21s  (22.0%)10.1%  lr: 0.038989  loss: 0.002470  eta: 0h11m  tot: 0h3m22s  (22.0%)11.3%  lr: 0.038849  loss: 0.002478  eta: 0h11m  tot: 0h3m24s  (22.3%)13.7%  lr: 0.038519  loss: 0.002558  eta: 0h11m  tot: 0h3m28s  (22.7%)14.5%  lr: 0.038408  loss: 0.002575  eta: 0h11m  tot: 0h3m30s  (22.9%)19.2%  lr: 0.037938  loss: 0.002543  eta: 0h11m  tot: 0h3m38s  (23.8%)19.7%  lr: 0.037918  loss: 0.002535  eta: 0h10m  tot: 0h3m39s  (23.9%)23.9%  lr: 0.037518  loss: 0.002569  eta: 0h10m  tot: 0h3m46s  (24.8%)%  lr: 0.037247  loss: 0.002598  eta: 0h10m  tot: 0h3m52s  (25.3%)27.3%  lr: 0.037197  loss: 0.002599  eta: 0h10m  tot: 0h3m53s  (25.5%)27.4%  lr: 0.037187  loss: 0.002600  eta: 0h10m  tot: 0h3m53s  (25.5%)27.6%  lr: 0.037137  loss: 0.002610  eta: 0h10m  tot: 0h3m53s  (25.5%)27.7%  lr: 0.037127  loss: 0.002614  eta: 0h10m  tot: 0h3m54s  (25.5%)28.3%  lr: 0.037067  loss: 0.002612  eta: 0h10m  tot: 0h3m54s  (25.7%)0.036997  loss: 0.002610  eta: 0h10m  tot: 0h3m56s  (25.8%)29.4%  lr: 0.036947  loss: 0.002611  eta: 0h10m  tot: 0h3m57s  (25.9%)%  lr: 0.036907  loss: 0.002620  eta: 0h10m  tot: 0h3m58s  (26.0%)30.2%  lr: 0.036887  loss: 0.002628  eta: 0h10m  tot: 0h3m58s  (26.0%)30.8%  lr: 0.036847  loss: 0.002634  eta: 0h10m  tot: 0h3m59s  (26.2%)31.8%  lr: 0.036787  loss: 0.002642  eta: 0h10m  tot: 0h4m0s  (26.4%)33.9%  lr: 0.036657  loss: 0.002650  eta: 0h10m  tot: 0h4m3s  (26.8%)34.1%  lr: 0.036657  loss: 0.002643  eta: 0h10m  tot: 0h4m3s  (26.8%)35.2%  lr: 0.036557  loss: 0.002666  eta: 0h10m  tot: 0h4m5s  (27.0%)35.6%  lr: 0.036527  loss: 0.002658  eta: 0h10m  tot: 0h4m6s  (27.1%)10m  tot: 0h4m9s  (27.5%)0h10m  tot: 0h4m18s  (28.4%)44.2%  lr: 0.035716  loss: 0.002632  eta: 0h10m  tot: 0h4m22s  (28.8%)44.5%  lr: 0.035686  loss: 0.002637  eta: 0h10m  tot: 0h4m22s  (28.9%)46.7%  lr: 0.035425  loss: 0.002662  eta: 0h10m  tot: 0h4m26s  (29.3%)47.1%  lr: 0.035415  loss: 0.002661  eta: 0h10m  tot: 0h4m27s  (29.4%)47.6%  lr: 0.035355  loss: 0.002657  eta: 0h10m  tot: 0h4m27s  (29.5%)48.0%  lr: 0.035345  loss: 0.002661  eta: 0h10m  tot: 0h4m28s  (29.6%)0h10m  tot: 0h4m29s  (29.7%)48.6%  lr: 0.035285  loss: 0.002665  eta: 0h10m  tot: 0h4m29s  (29.7%)49.5%  lr: 0.035205  loss: 0.002665  eta: 0h10m  tot: 0h4m31s  (29.9%)52.1%  lr: 0.034965  loss: 0.002651  eta: 0h10m  tot: 0h4m35s  (30.4%)0.034425  loss: 0.002641  eta: 0h10m  tot: 0h4m41s  (31.1%)56.0%  lr: 0.034354  loss: 0.002640  eta: 0h10m  tot: 0h4m42s  (31.2%)0.034154  loss: 0.002647  eta: 0h9m  tot: 0h4m46s  (31.6%)%  lr: 0.034154  loss: 0.002648  eta: 0h9m  tot: 0h4m46s  (31.6%)58.4%  lr: 0.034144  loss: 0.002649  eta: 0h9m  tot: 0h4m46s  (31.7%)%  lr: 0.034024  loss: 0.002656  eta: 0h9m  tot: 0h4m49s  (32.0%)60.4%  lr: 0.033934  loss: 0.002650  eta: 0h9m  tot: 0h4m50s  (32.1%)60.9%  lr: 0.033854  loss: 0.002648  eta: 0h9m  tot: 0h4m51s  (32.2%)62.0%  lr: 0.033764  loss: 0.002652  eta: 0h9m  tot: 0h4m53s  (32.4%)62.7%  lr: 0.033694  loss: 0.002646  eta: 0h9m  tot: 0h4m54s  (32.5%)64.3%  lr: 0.033494  loss: 0.002644  eta: 0h9m  tot: 0h4m57s  (32.9%)68.8%  lr: 0.032973  loss: 0.002657  eta: 0h9m  tot: 0h5m6s  (33.8%)69.6%  lr: 0.032933  loss: 0.002657  eta: 0h9m  tot: 0h5m7s  (33.9%)69.7%  lr: 0.032913  loss: 0.002655  eta: 0h9m  tot: 0h5m7s  (33.9%)69.9%  lr: 0.032893  loss: 0.002656  eta: 0h9m  tot: 0h5m8s  (34.0%)70.4%  lr: 0.032853  loss: 0.002657  eta: 0h9m  tot: 0h5m9s  (34.1%)75.3%  lr: 0.032433  loss: 0.002653  eta: 0h9m  tot: 0h5m17s  (35.1%)%  lr: 0.032362  loss: 0.002654  eta: 0h9m  tot: 0h5m18s  (35.1%)80.6%  lr: 0.031792  loss: 0.002642  eta: 0h9m  tot: 0h5m27s  (36.1%)81.5%  lr: 0.031752  loss: 0.002645  eta: 0h9m  tot: 0h5m29s  (36.3%)%  lr: 0.031712  loss: 0.002645  eta: 0h9m  tot: 0h5m29s  (36.3%)81.9%  lr: 0.031672  loss: 0.002644  eta: 0h9m  tot: 0h5m29s  (36.4%)82.5%  lr: 0.031652  loss: 0.002646  eta: 0h9m  tot: 0h5m30s  (36.5%)82.6%  lr: 0.031642  loss: 0.002645  eta: 0h9m  tot: 0h5m30s  (36.5%)83.4%  lr: 0.031562  loss: 0.002648  eta: 0h9m  tot: 0h5m32s  (36.7%)9m  tot: 0h5m34s  (36.9%)85.6%  lr: 0.031372  loss: 0.002651  eta: 0h9m  tot: 0h5m35s  (37.1%)9m  tot: 0h5m40s  (37.6%)89.1%  lr: 0.031091  loss: 0.002647  eta: 0h9m  tot: 0h5m41s  (37.8%)89.8%  lr: 0.031021  loss: 0.002642  eta: 0h9m  tot: 0h5m42s  (38.0%)90.0%  lr: 0.031011  loss: 0.002641  eta: 0h9m  tot: 0h5m43s  (38.0%)90.1%  lr: 0.030981  loss: 0.002641  eta: 0h9m  tot: 0h5m43s  (38.0%)%  lr: 0.030941  loss: 0.002641  eta: 0h9m  tot: 0h5m43s  (38.1%)90.8%  lr: 0.030871  loss: 0.002644  eta: 0h9m  tot: 0h5m44s  (38.2%)92.0%  lr: 0.030761  loss: 0.002635  eta: 0h8m  tot: 0h5m46s  (38.4%)92.2%  lr: 0.030761  loss: 0.002634  eta: 0h8m  tot: 0h5m46s  (38.4%)93.4%  lr: 0.030661  loss: 0.002630  eta: 0h8m  tot: 0h5m48s  (38.7%)93.9%  lr: 0.030591  loss: 0.002627  eta: 0h8m  tot: 0h5m49s  (38.8%)94.3%  lr: 0.030561  loss: 0.002627  eta: 0h8m  tot: 0h5m50s  (38.9%)94.6%  lr: 0.030561  loss: 0.002626  eta: 0h8m  tot: 0h5m50s  (38.9%)95.8%  lr: 0.030471  loss: 0.002621  eta: 0h8m  tot: 0h5m52s  (39.2%)96.0%  lr: 0.030421  loss: 0.002622  eta: 0h8m  tot: 0h5m52s  (39.2%)m  tot: 0h5m58s  (39.9%)\n",
      " ---+++                Epoch    1 Train error : 0.00267679 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.020100  loss: 0.001884  eta: 0h5m  tot: 0h8m50s  (60.0%)%  lr: 0.029950  loss: 0.002252  eta: 0h6m  tot: 0h6m2s  (40.2%)2.1%  lr: 0.029900  loss: 0.001723  eta: 0h8m  tot: 0h6m4s  (40.4%)5.7%  lr: 0.029530  loss: 0.001844  eta: 0h8m  tot: 0h6m11s  (41.1%)7.2%  lr: 0.029449  loss: 0.001875  eta: 0h8m  tot: 0h6m13s  (41.4%)9.5%  lr: 0.029309  loss: 0.001857  eta: 0h8m  tot: 0h6m17s  (41.9%)9.8%  lr: 0.029279  loss: 0.001848  eta: 0h8m  tot: 0h6m18s  (42.0%)9.9%  lr: 0.029259  loss: 0.001845  eta: 0h8m  tot: 0h6m18s  (42.0%)10.0%  lr: 0.029249  loss: 0.001843  eta: 0h8m  tot: 0h6m18s  (42.0%)11.1%  lr: 0.029089  loss: 0.001788  eta: 0h8m  tot: 0h6m20s  (42.2%)13.5%  lr: 0.028769  loss: 0.001823  eta: 0h8m  tot: 0h6m24s  (42.7%)13.6%  lr: 0.028759  loss: 0.001817  eta: 0h8m  tot: 0h6m24s  (42.7%)16.5%  lr: 0.028378  loss: 0.001789  eta: 0h8m  tot: 0h6m29s  (43.3%)16.9%  lr: 0.028338  loss: 0.001787  eta: 0h8m  tot: 0h6m30s  (43.4%)8m  tot: 0h6m31s  (43.5%)19.4%  lr: 0.028058  loss: 0.001806  eta: 0h8m  tot: 0h6m34s  (43.9%)20.2%  lr: 0.027958  loss: 0.001818  eta: 0h8m  tot: 0h6m36s  (44.0%)24.7%  lr: 0.027508  loss: 0.001859  eta: 0h7m  tot: 0h6m44s  (44.9%)27.1%  lr: 0.027197  loss: 0.001867  eta: 0h7m  tot: 0h6m48s  (45.4%)27.3%  lr: 0.027187  loss: 0.001869  eta: 0h7m  tot: 0h6m48s  (45.5%)%  lr: 0.027107  loss: 0.001881  eta: 0h7m  tot: 0h6m49s  (45.6%)28.5%  lr: 0.027007  loss: 0.001880  eta: 0h7m  tot: 0h6m50s  (45.7%)29.9%  lr: 0.026887  loss: 0.001869  eta: 0h7m  tot: 0h6m52s  (46.0%)30.3%  lr: 0.026867  loss: 0.001873  eta: 0h7m  tot: 0h6m53s  (46.1%)31.3%  lr: 0.026797  loss: 0.001868  eta: 0h7m  tot: 0h6m54s  (46.3%)31.4%  lr: 0.026797  loss: 0.001870  eta: 0h7m  tot: 0h6m54s  (46.3%)31.6%  lr: 0.026777  loss: 0.001872  eta: 0h7m  tot: 0h6m55s  (46.3%)32.0%  lr: 0.026737  loss: 0.001870  eta: 0h7m  tot: 0h6m56s  (46.4%)34.3%  lr: 0.026607  loss: 0.001882  eta: 0h7m  tot: 0h6m59s  (46.9%)36.4%  lr: 0.026436  loss: 0.001874  eta: 0h7m  tot: 0h7m2s  (47.3%)m  tot: 0h7m25s  (49.9%)50.0%  lr: 0.024985  loss: 0.001890  eta: 0h7m  tot: 0h7m26s  (50.0%)%  lr: 0.024885  loss: 0.001885  eta: 0h7m  tot: 0h7m28s  (50.2%)52.0%  lr: 0.024705  loss: 0.001884  eta: 0h7m  tot: 0h7m30s  (50.4%)%  lr: 0.024695  loss: 0.001883  eta: 0h7m  tot: 0h7m30s  (50.4%)52.8%  lr: 0.024635  loss: 0.001887  eta: 0h7m  tot: 0h7m31s  (50.6%)%  lr: 0.024615  loss: 0.001885  eta: 0h6m  tot: 0h7m33s  (50.8%)54.5%  lr: 0.024535  loss: 0.001887  eta: 0h6m  tot: 0h7m33s  (50.9%)0.024495  loss: 0.001882  eta: 0h6m  tot: 0h7m34s  (51.0%)6m  tot: 0h7m34s  (51.0%)55.3%  lr: 0.024465  loss: 0.001880  eta: 0h6m  tot: 0h7m35s  (51.1%)56.1%  lr: 0.024415  loss: 0.001884  eta: 0h6m  tot: 0h7m36s  (51.2%)56.8%  lr: 0.024354  loss: 0.001881  eta: 0h6m  tot: 0h7m37s  (51.4%)6m  tot: 0h7m38s  (51.4%)58.2%  lr: 0.024224  loss: 0.001874  eta: 0h6m  tot: 0h7m40s  (51.6%)60.2%  lr: 0.024034  loss: 0.001875  eta: 0h6m  tot: 0h7m43s  (52.0%)61.6%  lr: 0.023944  loss: 0.001876  eta: 0h6m  tot: 0h7m45s  (52.3%)62.0%  lr: 0.023864  loss: 0.001877  eta: 0h6m  tot: 0h7m46s  (52.4%)6m  tot: 0h7m46s  (52.4%)63.7%  lr: 0.023594  loss: 0.001892  eta: 0h6m  tot: 0h7m49s  (52.7%)64.8%  lr: 0.023564  loss: 0.001895  eta: 0h6m  tot: 0h7m51s  (53.0%)%  lr: 0.023464  loss: 0.001908  eta: 0h6m  tot: 0h7m53s  (53.2%)67.1%  lr: 0.023303  loss: 0.001902  eta: 0h6m  tot: 0h7m55s  (53.4%)69.4%  lr: 0.023093  loss: 0.001890  eta: 0h6m  tot: 0h7m59s  (53.9%)71.8%  lr: 0.022883  loss: 0.001887  eta: 0h6m  tot: 0h8m3s  (54.4%)72.8%  lr: 0.022753  loss: 0.001890  eta: 0h6m  tot: 0h8m4s  (54.6%)74.8%  lr: 0.022613  loss: 0.001877  eta: 0h6m  tot: 0h8m8s  (55.0%)75.9%  lr: 0.022543  loss: 0.001879  eta: 0h6m  tot: 0h8m9s  (55.2%)76.1%  lr: 0.022513  loss: 0.001880  eta: 0h6m  tot: 0h8m10s  (55.2%)  lr: 0.022403  loss: 0.001874  eta: 0h6m  tot: 0h8m11s  (55.4%)77.1%  lr: 0.022403  loss: 0.001873  eta: 0h6m  tot: 0h8m12s  (55.4%)78.1%  lr: 0.022272  loss: 0.001878  eta: 0h6m  tot: 0h8m14s  (55.6%)78.7%  lr: 0.022232  loss: 0.001873  eta: 0h6m  tot: 0h8m15s  (55.7%)79.2%  lr: 0.022182  loss: 0.001875  eta: 0h6m  tot: 0h8m15s  (55.8%)81.2%  lr: 0.021982  loss: 0.001876  eta: 0h6m  tot: 0h8m18s  (56.2%)  lr: 0.021872  loss: 0.001877  eta: 0h6m  tot: 0h8m20s  (56.4%)81.9%  lr: 0.021872  loss: 0.001881  eta: 0h6m  tot: 0h8m20s  (56.4%)82.0%  lr: 0.021852  loss: 0.001879  eta: 0h6m  tot: 0h8m20s  (56.4%)m  tot: 0h8m21s  (56.5%)0.001874  eta: 0h6m  tot: 0h8m23s  (56.7%)86.5%  lr: 0.021382  loss: 0.001882  eta: 0h6m  tot: 0h8m28s  (57.3%)88.3%  lr: 0.021201  loss: 0.001888  eta: 0h6m  tot: 0h8m31s  (57.7%)%  lr: 0.021131  loss: 0.001885  eta: 0h5m  tot: 0h8m32s  (57.8%)90.8%  lr: 0.020931  loss: 0.001885  eta: 0h5m  tot: 0h8m35s  (58.2%)92.7%  lr: 0.020771  loss: 0.001884  eta: 0h5m  tot: 0h8m38s  (58.5%)0.020421  loss: 0.001880  eta: 0h5m  tot: 0h8m44s  (59.3%)96.5%  lr: 0.020411  loss: 0.001879  eta: 0h5m  tot: 0h8m44s  (59.3%)  lr: 0.020300  loss: 0.001882  eta: 0h5m  tot: 0h8m46s  (59.5%)98.3%  lr: 0.020210  loss: 0.001882  eta: 0h5m  tot: 0h8m48s  (59.7%)\n",
      " ---+++                Epoch    2 Train error : 0.00189600 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010010  loss: 0.001547  eta: 0h2m  tot: 0h11m44s  (80.0%)%  lr: 0.019540  loss: 0.001341  eta: 0h5m  tot: 0h8m59s  (60.9%)5.4%  lr: 0.019419  loss: 0.001414  eta: 0h5m  tot: 0h9m1s  (61.1%)5.9%  lr: 0.019369  loss: 0.001435  eta: 0h5m  tot: 0h9m2s  (61.2%)%  lr: 0.019199  loss: 0.001440  eta: 0h5m  tot: 0h9m4s  (61.5%)9.6%  lr: 0.019069  loss: 0.001494  eta: 0h5m  tot: 0h9m7s  (61.9%)9.8%  lr: 0.019039  loss: 0.001489  eta: 0h5m  tot: 0h9m8s  (62.0%)%)12.4%  lr: 0.018739  loss: 0.001496  eta: 0h5m  tot: 0h9m12s  (62.5%)14.5%  lr: 0.018619  loss: 0.001489  eta: 0h5m  tot: 0h9m16s  (62.9%)14.6%  lr: 0.018609  loss: 0.001489  eta: 0h5m  tot: 0h9m16s  (62.9%)17.2%  lr: 0.018358  loss: 0.001498  eta: 0h5m  tot: 0h9m20s  (63.4%)19.3%  lr: 0.018048  loss: 0.001532  eta: 0h5m  tot: 0h9m24s  (63.9%)19.4%  lr: 0.018028  loss: 0.001538  eta: 0h5m  tot: 0h9m24s  (63.9%)20.4%  lr: 0.017928  loss: 0.001548  eta: 0h5m  tot: 0h9m26s  (64.1%)20.7%  lr: 0.017888  loss: 0.001551  eta: 0h5m  tot: 0h9m26s  (64.1%)22.1%  lr: 0.017708  loss: 0.001538  eta: 0h5m  tot: 0h9m29s  (64.4%)4m  tot: 0h9m37s  (65.3%)28.7%  lr: 0.016997  loss: 0.001499  eta: 0h4m  tot: 0h9m41s  (65.7%)28.8%  lr: 0.016987  loss: 0.001497  eta: 0h4m  tot: 0h9m41s  (65.8%)4m  tot: 0h9m45s  (66.2%)31.8%  lr: 0.016727  loss: 0.001487  eta: 0h4m  tot: 0h9m46s  (66.4%)32.2%  lr: 0.016677  loss: 0.001491  eta: 0h4m  tot: 0h9m47s  (66.4%)32.8%  lr: 0.016617  loss: 0.001495  eta: 0h4m  tot: 0h9m48s  (66.6%)  lr: 0.016036  loss: 0.001509  eta: 0h4m  tot: 0h9m58s  (67.8%)41.8%  lr: 0.015796  loss: 0.001524  eta: 0h4m  tot: 0h10m3s  (68.4%)%  lr: 0.015425  loss: 0.001511  eta: 0h4m  tot: 0h10m9s  (69.1%)46.3%  lr: 0.015395  loss: 0.001505  eta: 0h4m  tot: 0h10m11s  (69.3%)%  lr: 0.015315  loss: 0.001509  eta: 0h4m  tot: 0h10m11s  (69.4%)47.4%  lr: 0.015235  loss: 0.001507  eta: 0h4m  tot: 0h10m13s  (69.5%)47.4%  lr: 0.015235  loss: 0.001507  eta: 0h4m  tot: 0h10m13s  (69.5%)49.2%  lr: 0.014985  loss: 0.001505  eta: 0h4m  tot: 0h10m16s  (69.8%)50.1%  lr: 0.014785  loss: 0.001502  eta: 0h4m  tot: 0h10m17s  (70.0%)0.014765  loss: 0.001501  eta: 0h4m  tot: 0h10m17s  (70.0%)50.6%  lr: 0.014705  loss: 0.001498  eta: 0h4m  tot: 0h10m18s  (70.1%)54.6%  lr: 0.014274  loss: 0.001512  eta: 0h4m  tot: 0h10m25s  (70.9%)56.0%  lr: 0.014174  loss: 0.001521  eta: 0h4m  tot: 0h10m27s  (71.2%)56.4%  lr: 0.014154  loss: 0.001519  eta: 0h4m  tot: 0h10m28s  (71.3%)57.6%  lr: 0.014014  loss: 0.001529  eta: 0h4m  tot: 0h10m30s  (71.5%)4m  tot: 0h10m31s  (71.7%)59.0%  lr: 0.013934  loss: 0.001530  eta: 0h4m  tot: 0h10m32s  (71.8%)59.6%  lr: 0.013874  loss: 0.001531  eta: 0h4m  tot: 0h10m33s  (71.9%)%  lr: 0.013654  loss: 0.001522  eta: 0h3m  tot: 0h10m38s  (72.4%)62.4%  lr: 0.013594  loss: 0.001524  eta: 0h3m  tot: 0h10m38s  (72.5%)62.8%  lr: 0.013524  loss: 0.001522  eta: 0h3m  tot: 0h10m39s  (72.6%)63.3%  lr: 0.013494  loss: 0.001523  eta: 0h3m  tot: 0h10m40s  (72.7%)66.7%  lr: 0.013223  loss: 0.001537  eta: 0h3m  tot: 0h10m46s  (73.3%)m  tot: 0h10m47s  (73.4%)0.013143  loss: 0.001546  eta: 0h3m  tot: 0h10m49s  (73.6%)69.9%  lr: 0.012943  loss: 0.001550  eta: 0h3m  tot: 0h10m52s  (74.0%)0.012643  loss: 0.001550  eta: 0h3m  tot: 0h10m59s  (74.7%)  tot: 0h11m0s  (74.7%)3m  tot: 0h11m1s  (74.9%)%  lr: 0.012453  loss: 0.001553  eta: 0h3m  tot: 0h11m2s  (75.0%)%  lr: 0.012262  loss: 0.001556  eta: 0h3m  tot: 0h11m4s  (75.3%)77.1%  lr: 0.012192  loss: 0.001560  eta: 0h3m  tot: 0h11m6s  (75.4%)77.7%  lr: 0.012152  loss: 0.001556  eta: 0h3m  tot: 0h11m6s  (75.5%)78.8%  lr: 0.012032  loss: 0.001557  eta: 0h3m  tot: 0h11m8s  (75.8%)79.9%  lr: 0.011932  loss: 0.001561  eta: 0h3m  tot: 0h11m10s  (76.0%)82.4%  lr: 0.011652  loss: 0.001561  eta: 0h3m  tot: 0h11m14s  (76.5%)82.8%  lr: 0.011592  loss: 0.001556  eta: 0h3m  tot: 0h11m15s  (76.6%)83.6%  lr: 0.011512  loss: 0.001556  eta: 0h3m  tot: 0h11m16s  (76.7%)85.2%  lr: 0.011331  loss: 0.001557  eta: 0h3m  tot: 0h11m19s  (77.0%)3m  tot: 0h11m19s  (77.1%)3m  tot: 0h11m22s  (77.4%)88.0%  lr: 0.011041  loss: 0.001552  eta: 0h3m  tot: 0h11m24s  (77.6%)88.4%  lr: 0.010941  loss: 0.001551  eta: 0h3m  tot: 0h11m25s  (77.7%)88.6%  lr: 0.010921  loss: 0.001553  eta: 0h3m  tot: 0h11m25s  (77.7%)89.1%  lr: 0.010871  loss: 0.001551  eta: 0h3m  tot: 0h11m26s  (77.8%)%  lr: 0.010711  loss: 0.001550  eta: 0h3m  tot: 0h11m30s  (78.3%)91.7%  lr: 0.010681  loss: 0.001549  eta: 0h3m  tot: 0h11m30s  (78.3%)91.9%  lr: 0.010661  loss: 0.001550  eta: 0h3m  tot: 0h11m31s  (78.4%)92.5%  lr: 0.010611  loss: 0.001548  eta: 0h3m  tot: 0h11m32s  (78.5%)94.0%  lr: 0.010461  loss: 0.001544  eta: 0h3m  tot: 0h11m35s  (78.8%)99.2%  lr: 0.010040  loss: 0.001546  eta: 0h2m  tot: 0h11m44s  (79.8%)  eta: 0h2m  tot: 0h11m44s  (79.9%)\n",
      " ---+++                Epoch    3 Train error : 0.00154964 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.000000  loss: 0.001348  eta: <1min   tot: 0h14m36s  (100.0%) lr: 0.009620  loss: 0.001479  eta: 0h2m  tot: 0h11m51s  (80.8%)7.7%  lr: 0.009159  loss: 0.001365  eta: 0h2m  tot: 0h11m58s  (81.5%)10.4%  lr: 0.008839  loss: 0.001336  eta: 0h2m  tot: 0h12m3s  (82.1%)11.5%  lr: 0.008779  loss: 0.001335  eta: 0h2m  tot: 0h12m4s  (82.3%)14.8%  lr: 0.008468  loss: 0.001334  eta: 0h2m  tot: 0h12m10s  (83.0%)15.2%  lr: 0.008368  loss: 0.001321  eta: 0h2m  tot: 0h12m11s  (83.0%)2m  tot: 0h12m11s  (83.1%)17.2%  lr: 0.008238  loss: 0.001327  eta: 0h2m  tot: 0h12m14s  (83.4%)17.8%  lr: 0.008178  loss: 0.001320  eta: 0h2m  tot: 0h12m15s  (83.6%)19.1%  lr: 0.008048  loss: 0.001335  eta: 0h2m  tot: 0h12m18s  (83.8%)20.3%  lr: 0.007948  loss: 0.001339  eta: 0h2m  tot: 0h12m20s  (84.1%)23.9%  lr: 0.007648  loss: 0.001380  eta: 0h2m  tot: 0h12m27s  (84.8%)24.0%  lr: 0.007618  loss: 0.001375  eta: 0h2m  tot: 0h12m27s  (84.8%)24.2%  lr: 0.007608  loss: 0.001377  eta: 0h2m  tot: 0h12m28s  (84.8%)24.8%  lr: 0.007568  loss: 0.001360  eta: 0h2m  tot: 0h12m28s  (85.0%)2m  tot: 0h12m31s  (85.3%)27.4%  lr: 0.007297  loss: 0.001360  eta: 0h2m  tot: 0h12m33s  (85.5%)29.5%  lr: 0.007067  loss: 0.001356  eta: 0h2m  tot: 0h12m36s  (85.9%)0.006947  loss: 0.001335  eta: 0h2m  tot: 0h12m40s  (86.3%)32.5%  lr: 0.006847  loss: 0.001333  eta: 0h1m  tot: 0h12m41s  (86.5%)34.4%  lr: 0.006577  loss: 0.001330  eta: 0h1m  tot: 0h12m45s  (86.9%)38.3%  lr: 0.006136  loss: 0.001332  eta: 0h1m  tot: 0h12m52s  (87.7%)40.0%  lr: 0.005986  loss: 0.001341  eta: 0h1m  tot: 0h12m54s  (88.0%)40.2%  lr: 0.005966  loss: 0.001339  eta: 0h1m  tot: 0h12m55s  (88.0%)40.7%  lr: 0.005946  loss: 0.001337  eta: 0h1m  tot: 0h12m56s  (88.1%)41.1%  lr: 0.005926  loss: 0.001344  eta: 0h1m  tot: 0h12m57s  (88.2%)42.1%  lr: 0.005746  loss: 0.001341  eta: 0h1m  tot: 0h12m58s  (88.4%)42.3%  lr: 0.005696  loss: 0.001341  eta: 0h1m  tot: 0h12m59s  (88.5%)%  lr: 0.005375  loss: 0.001329  eta: 0h1m  tot: 0h13m5s  (89.1%)46.2%  lr: 0.005325  loss: 0.001322  eta: 0h1m  tot: 0h13m6s  (89.2%)47.3%  lr: 0.005195  loss: 0.001322  eta: 0h1m  tot: 0h13m8s  (89.5%)48.4%  lr: 0.005005  loss: 0.001317  eta: 0h1m  tot: 0h13m10s  (89.7%)53.7%  lr: 0.004555  loss: 0.001323  eta: 0h1m  tot: 0h13m19s  (90.7%)%  lr: 0.004475  loss: 0.001322  eta: 0h1m  tot: 0h13m21s  (91.0%)55.8%  lr: 0.004364  loss: 0.001323  eta: 0h1m  tot: 0h13m23s  (91.2%)1m  tot: 0h13m23s  (91.2%)56.3%  lr: 0.004334  loss: 0.001324  eta: 0h1m  tot: 0h13m23s  (91.3%)56.4%  lr: 0.004314  loss: 0.001323  eta: 0h1m  tot: 0h13m24s  (91.3%)58.0%  lr: 0.004214  loss: 0.001321  eta: 0h1m  tot: 0h13m27s  (91.6%)58.7%  lr: 0.004094  loss: 0.001318  eta: 0h1m  tot: 0h13m28s  (91.7%)63.5%  lr: 0.003614  loss: 0.001332  eta: 0h1m  tot: 0h13m36s  (92.7%)63.6%  lr: 0.003604  loss: 0.001335  eta: 0h1m  tot: 0h13m36s  (92.7%)64.5%  lr: 0.003444  loss: 0.001337  eta: 0h1m  tot: 0h13m38s  (92.9%)64.7%  lr: 0.003403  loss: 0.001336  eta: 0h1m  tot: 0h13m38s  (92.9%)65.5%  lr: 0.003263  loss: 0.001334  eta: 0h1m  tot: 0h13m40s  (93.1%)%  lr: 0.002833  loss: 0.001342  eta: <1min   tot: 0h13m46s  (93.8%)69.8%  lr: 0.002743  loss: 0.001343  eta: <1min   tot: 0h13m47s  (94.0%)71.1%  lr: 0.002623  loss: 0.001339  eta: <1min   tot: 0h13m50s  (94.2%)71.6%  lr: 0.002563  loss: 0.001338  eta: <1min   tot: 0h13m51s  (94.3%)72.2%  lr: 0.002493  loss: 0.001335  eta: <1min   tot: 0h13m52s  (94.4%)76.4%  lr: 0.002092  loss: 0.001337  eta: <1min   tot: 0h13m59s  (95.3%)0.001339  eta: <1min   tot: 0h14m2s  (95.6%)79.4%  lr: 0.001802  loss: 0.001338  eta: <1min   tot: 0h14m4s  (95.9%)80.4%  lr: 0.001762  loss: 0.001343  eta: <1min   tot: 0h14m6s  (96.1%)81.3%  lr: 0.001652  loss: 0.001339  eta: <1min   tot: 0h14m7s  (96.3%)83.2%  lr: 0.001502  loss: 0.001341  eta: <1min   tot: 0h14m11s  (96.6%)0.001340  eta: <1min   tot: 0h14m15s  (97.2%)85.9%  lr: 0.001191  loss: 0.001339  eta: <1min   tot: 0h14m15s  (97.2%)0.001337  eta: <1min   tot: 0h14m18s  (97.5%)88.1%  lr: 0.000941  loss: 0.001337  eta: <1min   tot: 0h14m19s  (97.6%)89.8%  lr: 0.000711  loss: 0.001335  eta: <1min   tot: 0h14m21s  (98.0%)90.8%  lr: 0.000581  loss: 0.001334  eta: <1min   tot: 0h14m23s  (98.2%)%  lr: 0.000410  loss: 0.001339  eta: <1min   tot: 0h14m27s  (98.6%)95.1%  lr: 0.000230  loss: 0.001338  eta: <1min   tot: 0h14m31s  (99.0%)95.2%  lr: 0.000230  loss: 0.001338  eta: <1min   tot: 0h14m31s  (99.0%)97.0%  lr: 0.000100  loss: 0.001343  eta: <1min   tot: 0h14m34s  (99.4%)\n",
      " ---+++                Epoch    4 Train error : 0.00137713 +++--- ���\n",
      "Saving model to file : starspace_embedding\n",
      "Saving model in tsv format : starspace_embedding.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!starspace train -trainFile \"data/prepared_train.tsv\" -model starspace_embedding \\\n",
    "-trainMode 3 -adagrad true -ngrams 1 -epoch 5 -dim 100 -similarity cosine -minCount 2 \\\n",
    "-verbose true -fileFormat labelDoc -negSearchLimit 10 -lr 0.05 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = dict()\n",
    "for line in open('starspace_embedding.tsv', encoding='utf-8'):\n",
    "    row = line.strip().split('\\t')\n",
    "    starspace_embeddings[row[0]] = np.array(row[1:], dtype=np.float32)\n",
    "\n",
    "#starspace_embeddings = ######### YOUR CODE HERE #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.520 | Hits@   1: 0.520\n",
      "DCG@   5: 0.615 | Hits@   5: 0.698\n",
      "DCG@  10: 0.634 | Hits@  10: 0.755\n",
      "DCG@ 100: 0.665 | Hits@ 100: 0.905\n",
      "DCG@ 500: 0.675 | Hits@ 500: 0.981\n",
      "DCG@1000: 0.677 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 82\t49\t88\t27\t4\t53\t78\t96\t72\t66\t5\t10\t20\t32\t75\t51\t9\t12\t55\t85\t87\t42\t7\t89\t100\t74\t91\t97\t60\t1\t58\t31\t86\t50\t77...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = 'data/prepared_test.tsv' ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.333333333333\n",
      "0.666666666667\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 82\t49\t88\t27\t4\t53\t78\t96\t72\t66\t5\t10\t20\t32\t75\t51\t9\t12\t55\t85\t87\t42\t7\t89\t100\t74\t91\t97\t60\t1\t58\t31\t86\t50\t77...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'nimish.sanghi@gmail.com'# EMAIL \n",
    "STUDENT_TOKEN = 'pVDZL5Ea9hwAKc2O'# TOKEN \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}